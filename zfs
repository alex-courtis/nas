#
# disk setup
#

Samsung has some problems with NCQ, which results in zpool scrub failures. See https://blog.cavelab.dev/2021/02/zfs-ssd-pool-problems/

Force disable it via cmdline:

```
libata.force=1.00:noncq,3.00:noncq,5.00:noncq,7.00:noncq
```

Find the ataX.00 identifiers via:
```sh
journalctl -b -2 | grep NCQ
```

#
# creation
#

# disk ids
ls /dev/disk/by-id/ | grep -E "ata-Sam|nvme-CT"

# show block size
# most SSDs show 512 however
#  AW states that it's unreliable
#  zfs doc agrees
# so use ashift=12
blockdev --getpbsz /dev/sda

# create pool
zpool create \
	-f \
	-m /srv/nfs \
	-o ashift=12 \
	pool \
	raidz \
	ata-Samsung_SSD_870_QVO_8TB_S5SSNF0WA05324A \
	ata-Samsung_SSD_870_QVO_8TB_S5SSNF0WA05327T \
	ata-Samsung_SSD_870_QVO_8TB_S5SSNF0WA05350J \
	ata-Samsung_SSD_870_QVO_8TB_S5SSNF0WA05377P
zpool create \
	-f \
	-m /srv/fast \
	-o ashift=12 \
	fast \
	nvme-CT1000T700SSD3_2321E6DA3BF8

# show history
zpool history

# verify pool status
zpool status -v

# datasets don't add value

# no need to set the cache file, default is below
#zpool set cachefile=/etc/zfs/zpool.cache pool

# AW advises against zfs-mount due to load order issues, use zfs-mount-generator
mkdir /etc/zfs/zfs-list.cache

# enable services
systemctl enable zfs.target
systemctl enable zfs-zed.service
systemctl start zfs-zed.service

# create the file for zedlet to write the FS list
touch /etc/zfs/zfs-list.cache/pool
touch /etc/zfs/zfs-list.cache/fast

# nudge it into action
zfs set canmount=off pool
zfs set canmount=on pool
cat /etc/zfs/zfs-list.cache/pool
zfs set canmount=off fast
zfs set canmount=on fast
cat /etc/zfs/zfs-list.cache/fast

# do the first import
zfs mount -a

# enable auto mounts
systemctl enable zfs-import.target
systemctl enable zfs-import-cache.service

# reboot to ensure everything mounts automatically

# enable periodic trimming
zpool set autotrim=on pool
zpool set autotrim=on fast

# trim now
zpool trim pool
zpool status -t pool
zpool trim fast
zpool status -t fast

# datasets to mount under pool nfs4 fsid=root
zfs create fast/doc
zfs set mountpoint=/srv/nfs/doc fast/doc
zfs create fast/download
zfs set mountpoint=/srv/nfs/download fast/download
zfs create fast/prn
zfs set mountpoint=/srv/nfs/prn fast/prn
zfs create fast/tmp
zfs set mountpoint=/srv/nfs/tmp fast/tmp

# also enable trim timer, as occasional trims are recommended by AW
systemctl enable zfs-trim-weekly@pool.timer
systemctl enable zfs-trim-weekly@fast.timer

# periodic scrubbing
systemctl enable zfs-scrub-weekly@pool.timer
systemctl enable zfs-scrub-weekly@fast.timer

# scrub now
systemctl start zfs-scrub@pool.service
systemctl start zfs-scrub@fast.service
zpool status -t pool

#
# enable nfs
#
systemctl enable nfs-server.service
systemctl enable zfs-share.service
systemctl start nfs-server.service
systemctl start zfs-share.service

# nfs4 root mounts at /
# nfs4 directories and crossmnt dataset mounts at /<name>
# nfs3 mounts at /srv/nfs
zfs set sharenfs="rw=@10.0.0.0/8,crossmnt,fsid=root" pool

# show
exportfs -v
showmount -e lord
zfs get sharenfs





# disable/stop all services
systemctl disable nfs-server.service
systemctl stop nfsdcld.service
systemctl stop nfs-server.service

systemctl disable zfs.target
systemctl disable zfs-zed.service
systemctl disable zfs-import.target
systemctl disable zfs-import-cache.service
systemctl disable zfs-share.service

systemctl stop zfs.target
systemctl stop zfs-zed.service
systemctl stop zfs-import.target
systemctl stop zfs-import-cache.service
systemctl stop zfs-share.service

# vim:ft=sh

